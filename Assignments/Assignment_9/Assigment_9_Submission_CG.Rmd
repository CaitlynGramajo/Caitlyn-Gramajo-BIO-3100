---
title: "Assignment 9"
author: "Caitlyn Gramajo"
date: "2024-03-24"
output:
  html_document:
    code_folding: hide
  'html_document:': default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message = FALSE)
```

## Assignment 9

## Goal

I will create a logistic regression model to predict whether or not a student will be admitted to Graduate School using the "GradSchool_Admissions.csv" data set

## Libaray

For the set up I used 6 libraries:

tidyverse

caret

easystats

modelr

MASS

GGally

```{r}
library(tidyverse)
library(caret)
library(easystats)
library(modelr)
library(MASS)
library(GGally)
```

## Data
```{r}
data<-read_csv("GradSchool_Admissions.csv")
glimpse(data)
colnames(data)
```

The Data set has 4 columns: 2 continues (GRE score and GPA), 1 logical (admit 1 is yes 0 is no), and 1 rank (1,2,3,4) with 1 meaning that the school is "top tier".

## Plots
```{r}
ggpairs(data)

data %>% 
  ggplot(aes(x=gpa, y=gre, color = as.factor(admit)))+
  geom_point()+
  ggtitle("GPA v GRE colored by admit")+
  theme_minimal()

ggplot(data, aes(x = gpa, fill = as.factor(admit))) +
  geom_density(alpha = 0.5) +
  labs(x = "GPA", y = "Density", fill = "Admit Status") +
  theme_minimal()+
  ggtitle("Density Curve of GPA grouped by Admit Status")

ggplot(data, aes(x = gre, fill = as.factor(admit))) +
  geom_density(alpha = 0.5) +
  labs(x = "GRE", y = "Density", fill = "Admit Status") +
  theme_minimal()+
  ggtitle("Density Curve of GRE grouped by Admit Status")

ggplot(data, aes(x = as.factor(rank), fill = as.factor(admit))) +
  geom_bar(alpha = 0.5) +
  labs(x = "Rank", y = "Count", fill = "Admit Status") +
  theme_minimal()+
  ggtitle("Count of Students Applying to Schools split by Rank and colored by their Admission Status")

```

## Analysis from the Plots

We can see from GPA v GRE colored by admit that there is more rejections than acceptations and that the acceptations are more spread out than the rejections. I wonder if the acceptance points that are lower correlate to lower rank schools. From the Density Curve of GPA grouped by Admit Status can see that the mean GPA of students who were admitted is higher than that of the GPA of students where rejected but the two curves are very similar. From the Density Curve of GRE grouped by Admit Status we can see that the mean gre of students who were admitted is a little higher than that of the GRE of students where rejected however the two density curves overlap so much that I am suspicious that GRE does not matter as much as gpa does. From the bar chart or the ranks of the schools colored by admit status we can see that all but rank 1 schools reject more than the accept I wonder if lower rank schools get more applicants.

## Splitting the data 

For testing any models I make I am going to use a subset of the data that the model was not trained on and have it predict the probability of the student being admitted. These data sets are train for the training data and test for the testing data.

```{r}
id<-createDataPartition(data$admit, p = 0.8, list = F) 
train<-data[id,]
test<-data[-id,]
```

## Making some modles

Model 1 (mod1) uses the students GRE score, GPA and the rank the school to predict if the student will be accepted.

Model 2 (mod2) uses the students GPA and the rank the school to predict if the student will be accepted.

Model 3 (mod3) uses the students GRE score and the rank the school to predict if the student will be accepted.

Model 4 (mod4) uses the students GRE score and GPA to predict if the student will be accepted.

Model 5 (mod5) uses the rank the school to predict if the student will be accepted.

Model 6 (mod6) uses all terms after applying step AIC to the most complex model meaning it could at most include the students GRE score, GPA, the rank the school and all interaction terms to predict if the student will be accepted.

```{r}

mod1<-glm(data = train, 
          formula = as.logical(admit) ~ gre + gpa + factor(rank),
          family = binomial)
summary(mod1)

mod2<-glm(data = train, 
          formula = as.logical(admit) ~ gpa + factor(rank),
          family = binomial)
summary(mod2)

mod3<-glm(data = train, 
          formula = as.logical(admit) ~ gre + factor(rank),
          family = binomial)
summary(mod3)

mod4<-glm(data = train, 
          formula = as.logical(admit) ~ gpa + gre,
          family = binomial)
summary(mod4)

mod5<-glm(data = train, 
          formula = as.logical(admit) ~ as.factor(rank),
          family = binomial)
summary(mod5)

full_mod <- glm(data=train,
            formula=admit ~ gpa * gre * as.factor(rank))

step <- MASS::stepAIC(full_mod,trace=0) 
mod6 <- glm(data=train,
            formula=step$formula,
            family = binomial)
summary(mod6)

```

Model 1:
admit = -3.519205 + 0.001774(gre) + 0.756903(gpa) -0.751753(rank 2) -1.280555(rank 3) -1.670293(rank 4)

All predictors are significant except for gre.

Model 2:
admit = -3.1820 + 0.9734(gpa) -0.7558(rank 2) -1.3053(rank 3) -1.7050(rank 4)

All predictors are significant.

Model 3:
admit = -1.487971 +0.002738(gre) -0.794483(rank 2) -1.265685(rank 3) -1.731208(rank 4)

All predictors are significant.

Model 4:
admit = -4.715236 + 0.789733(gpa) + 0.002097(gpa)
All predictors are significant except for gre.

Model 5
admit = 0.1892 -0.8195(rank 2) -1.3164(rank 3) -1.8207(rank 4)
All predictors except for the intercept are significant.

Model 6
admit = -3.519205 + 0.001774(gre) + 0.756903(gpa) -0.751753(rank 2) -1.280555(rank 3) -1.670293(rank 4)

All predictors are significant except for gre. It is also the exact same model. This means that none of the interaction terms are significant enough to keep in.

## Comparing the Models

```{r}
compare_performance(mod1,mod2,mod3,mod4,mod5,mod6) %>% plot
compare_performance(mod1,mod2,mod3,mod4,mod5,mod6)
```

No model is the best in all categories but I choose Model 1 which is the same as Model 6 (which means that none of the interaction terms were significant interesting enough) because it has the lowest AIC and AICc dispite being the most complex. All models have the same Sigma but model 1 gets more predictions correct than the other models and has the lowest RMSE of the. It also explains the most this is shown by the the R2 value.

## Checking Model 1

```{r}
check_model(mod1)
```

Looking at the residuals we can see that they are normally distributed very strongly up until about 2 but even then they are not too far away. We can see in the influential observations plot that are no observations with high leverage meaning that no one observation has more leverage than the others. We can also see that all of the predictors are within the green zone for colinearity meaning they are not influencing each other too much. This is good because our predictors must be independent.

## Using Model 1 to predict with the test data set

```{r}
test$pred <- predict(mod1, newdata = test, type = "response")

#Plotting them
test %>% 
  ggplot(aes( x = gpa, y = pred, color = as.factor(rank)))+
  geom_point(alpha = 0.55)+
  geom_smooth()+
  theme_minimal()+
  ggtitle("Admission Predictons")

#How many are correct
test1<-
  test %>% 
  mutate(outcome = case_when(pred >= 0.5 ~ "Accept",
                             pred < 0.5 ~ "Reject")) %>% 
  mutate(correct = case_when(admit == "1" & outcome == "Accept" ~ TRUE,
                             admit != "1" & outcome == "Reject" ~ TRUE,
                             TRUE ~ FALSE))

#How many are correct?
correct_amount<-
  test1 %>% 
  pluck("correct") %>% 
  sum()/nrow(test) 


#Plotting predictions and coloring by whether or not they are correct
test1 %>% 
  ggplot(aes( x = gpa, y = pred, color = as.factor(correct)))+
  geom_point(alpha = 0.55)+
  geom_smooth()+
  theme_minimal()+
  ggtitle("Predictions colored by correct")

```

After using the test data set and plotting them we see that rank 1 school have the most variation in their predictions. Investigating into how many of these ne predictions are correct with a prediction value of 0.5 or higher being acceptions and less than 0.5 being rejection we find that `r correct_amount` of the predictions are correct. Plotting the predictions colored by whether they were correct or not shows that more of them are correct.

## Conclusion

None of the models I made had Percent correct prediction of more than 0.62 which is not great even the most complex model only had a PCP of 0.610 this leads me to believe that there are other predictors that could help my model preform better. I am thinking maybe income, location, and school of graduations may be helpful predictors to try in the future.